{"nbformat_minor": 0, "cells": [{"source": "#Hello Spark!!!\nApache Spark is a fast and general-purpose cluster computing system. It provides high-level APIs in Java, Scala, Python and R, and an optimized engine that supports general execution graphs. It also supports a rich set of higher-level tools including Spark SQL for SQL and structured data processing, MLlib for machine learning, GraphX for graph processing, and Spark Streaming.\n\n<img src='https://github.com/carloapp2/SparkPOT/blob/master/spark.png?raw=true' width=\"80%\" height=\"80%\"></img>\n\nThis notebook will show you some basic concepts to start working with Apache Spark including:\n\n- Understanding Spark Context\n- Creating Resilient Distributed Datasets (RDD)\n- Performing Data Transformations\n- Loading Data Files to use with Spark\n\n####Tool Tips:\n- Notice the navigation and command buttons at top of the notebook. Press Play & Stop buttons to execute code and interupt execution.\n- Notice each cell has type. (Markdown, Code, Etc) This cell is a Markdown cell which is simply HTML informational vs Code cell allows you to execute against spark.\n- Notice each cell has desigination, for eample In [n]: the number is cell number. When you see In [*]: that means the cell is executing\n- To see all methonds available for object you can use Tab key Example Enter \"SC.\" press Tab and a drop down will appear.\n- To execute code in active cell press play button at top or you can use short cut keys Shift-Enter, Ctrl-Enter", "cell_type": "markdown", "metadata": {}}, {"source": "###Spark Driver and Workers programs\nA Spark program has a driver program and a workers program. Worker programs run on cluster nodes or in local threads. RDDs are distributed\u001d across workers. \n\n<img src='https://github.com/carloapp2/SparkPOT/blob/master/Spark%20Architecture.png?raw=true' width=\"80%\" height=\"80%\"></img>\n\n###Python Spark (pySpark)\nWe are using the Python programming interface to Spark pySpark. \npySpark provides an easy-to-use programming abstraction and parallel runtime.\n\n###Spark Context\nApache Spark driver application uses a context allow a programming interface to interact with the driver application. This is know as a Spark Context which supports Python, Scala and Java programming languages. The SparkContext object tells Spark how and where to access a cluster.<br>\n<font color=\"red\">This lab uses IBM's fully managed cloud based notebook enviornment, so the spark context is predefined for you.</font><br>\n\nIn other enviornments you would need to pick an interprerter (i.e. pyspark for python) and create a Spark Config object to initilize a Spark Context. <br>\n\nExample:<br>\nfrom pyspark import SparkContext, SparkConf<br>\nconf = SparkConf().setAppName(appName).setMaster(master)<br>\nsc = SparkContext(conf=conf)<br>", "cell_type": "markdown", "metadata": {}}, {"execution_count": 20, "cell_type": "code", "metadata": {"collapsed": false, "trusted": false}, "outputs": [{"execution_count": 20, "metadata": {}, "data": {"text/plain": "<pyspark.context.SparkContext at 0x7f5c64317050>"}, "output_type": "execute_result"}], "source": "#Execute Spark Context to see if its active in cluster\nsc"}, {"execution_count": 21, "cell_type": "code", "metadata": {"collapsed": false, "trusted": false}, "outputs": [{"execution_count": 21, "metadata": {}, "data": {"text/plain": "u'1.6.0'"}, "output_type": "execute_result"}], "source": "#Execute to get the version of the spark driver application\n\n#Note: There is different versions of spark which support additional \n#functionality such as DataFrames, Streaming and Machine Learning.\nsc.version"}, {"source": "##Resilient Distributed Datasets\n\nApache Spark uses an abstraction for working with data called RDDs - Resilient Distributed Datasets. An RDD is an immutable fault-tolerant collection of elements that can be operated on in parallel. In Apache Spark all work is expressed by either creating new RDDs, transforming existing RDDs or using RDDs to compute results. When working with RDDs, the Spark Driver application automatically distributes the work accross your cluster.\n\n####You can construct RDDs by parallelizing existing Python collections (lists) or by transforming an existing RDDs or from files in HDFS or any other storage system. \n\n###Understanding Lazy Evaluations...\nRDDs have actions, which return values, and transformations, which return pointers to new RDDs. Transformations are transformations that do not initiate execution on the cluster. A transformation is mapped in a Digital Acrylic Graph (DAG) which is used to optimize execution on the cluster which occurs at time of an action.", "cell_type": "markdown", "metadata": {}}, {"execution_count": 23, "cell_type": "code", "metadata": {"collapsed": true, "trusted": false}, "outputs": [], "source": "#Create an RDD from Python collection of numbers\n\n#Create a Python collection\nx = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n\n#Place the collection into an rdd called x_nbr_rdd using parallelize\nx_nbr_rdd = sc.parallelize(x)"}, {"source": "####Notice no return occurs with sc.parallelize()\nThis means sc.parallelize didn't compute a result, so its a transformation. Spark only recorded how to create the RDD.", "cell_type": "markdown", "metadata": {}}, {"execution_count": 24, "cell_type": "code", "metadata": {"collapsed": false, "trusted": false}, "outputs": [{"execution_count": 24, "metadata": {}, "data": {"text/plain": "1"}, "output_type": "execute_result"}], "source": "#Execute an Action and return the first element\nx_nbr_rdd.first()"}, {"source": "####Notice you get a return on .first()\nThis means .first() is an Action. Spark executed all transformations to compute the result of .first().", "cell_type": "markdown", "metadata": {}}, {"execution_count": 25, "cell_type": "code", "metadata": {"collapsed": false, "trusted": false}, "outputs": [{"execution_count": 25, "metadata": {}, "data": {"text/plain": "[1, 2, 3, 4, 5]"}, "output_type": "execute_result"}], "source": "#Execute a Action and take first 5 elements\nx_nbr_rdd.take(5)"}, {"execution_count": 28, "cell_type": "code", "metadata": {"collapsed": false, "trusted": false}, "outputs": [{"execution_count": 28, "metadata": {}, "data": {"text/plain": "['Hello Human', 'My Name is Spark']"}, "output_type": "execute_result"}], "source": "#Create an RDD from Python collection of strings\n\n#Create a Python collection\ny = [\"Hello Human\", \"My Name is Spark\"]\n\n#Place the string value into an rdd called y_str_rdd\ny_str_rdd = sc.parallelize(y)\n\n#Return the first value in yoru RDD - Action\ny_str_rdd.take(2)"}, {"source": "##Data Transformations\nAs you can see, you created a string \"Hello Human\" and you returned value that was parallelized into RDD first element. If we wanted to work with a corpus of words and run analysis on strings to filter out words, then you would need to map each word into an RDD element.  ", "cell_type": "markdown", "metadata": {}}, {"source": "###Some common Transformation Functions\n\n- <b>map(func):</b> return a new distributed dataset formed by passing each element of the source through a function func\n- <b>filter(func):</b> return a new dataset formed by selecting those elements of the source on which func returns true\n- <b>distinct([numTasks])):</b> return a new dataset that contains the distinct elements of the source dataset\n- <b>flatMap(func):</b> similar to map, but each input item can be mapped to 0 or more output items (so func should return a Seq rather than a single item)", "cell_type": "markdown", "metadata": {}}, {"execution_count": 29, "cell_type": "code", "metadata": {"collapsed": false, "trusted": false}, "outputs": [{"execution_count": 29, "metadata": {}, "data": {"text/plain": "\"Hello Humman. I'm Apache Spark and I love running analysis on data.\""}, "output_type": "execute_result"}], "source": "#Create RDD named Words\nWords = [\"Hello Humman. I'm Apache Spark and I love running analysis on data.\"]\nwords_rd = sc.parallelize(Words)\nwords_rd.first()"}, {"source": "###Review: Python lambda Functions\n\n- Small anonymous functions (not bound to a name) \n\n- <b>lambda a , b : a + b</b> returns the sum of its two arguments\n\n- Can use lambda functions wherever function objects are required\n- Restricted to a single expression", "cell_type": "markdown", "metadata": {}}, {"execution_count": 30, "cell_type": "code", "metadata": {"collapsed": false, "trusted": false}, "outputs": [{"execution_count": 30, "metadata": {}, "data": {"text/plain": "['Hello',\n 'Humman.',\n \"I'm\",\n 'Apache',\n 'Spark',\n 'and',\n 'I',\n 'love',\n 'running',\n 'analysis',\n 'on',\n 'data.']"}, "output_type": "execute_result"}], "source": "Words_rd2 = words_rd.map(lambda line: line.split(\" \"))\nWords_rd2.first()"}, {"execution_count": 31, "cell_type": "code", "metadata": {"collapsed": false, "trusted": false}, "outputs": [{"execution_count": 31, "metadata": {}, "data": {"text/plain": "['Hello', 'Humman.', \"I'm\"]"}, "output_type": "execute_result"}], "source": "#Transform RDD Words into new RDD split on Space character.\nwords_rd2 = words_rd.flatMap(lambda line: line.split(\" \"))\nwords_rd2.take(3)"}, {"source": "###Filter Function\nThe filter command creates a new RDD from another RDD based on a filter criteria.\n\nfilter syntax is .filter(lambda line: \"Filter Criteria Value\" in line) \n\nHint: Use a simple python print command to add string to your spark results and run multiple actions in single cell.", "cell_type": "markdown", "metadata": {}}, {"execution_count": 32, "cell_type": "code", "metadata": {"collapsed": false, "trusted": false}, "outputs": [{"output_type": "stream", "name": "stdout", "text": "The count of words Hello\nIs: 1\n"}], "source": "#Count number of \"Hello\" words\n\n#Create a new RDD z_str3_rdd for all \"Hello\" words in corpus of words \nwords_rd3 = words_rd2.filter(lambda line: \"Hello\" in line) \n\n#Print count of values in the new RDD which represents number of \"Hello\" words in corpus\nprint \"The count of words \" + str(words_rd3.first())\nprint \"Is: \" + str(words_rd3.count())"}, {"source": "###Computations\nUsing Python and Spark functions to perform basic analytics on your data. Let investigate how we can sum a couple elements in a string.", "cell_type": "markdown", "metadata": {}}, {"execution_count": 33, "cell_type": "code", "metadata": {"collapsed": false, "trusted": false}, "outputs": [{"execution_count": 33, "metadata": {}, "data": {"text/plain": "13"}, "output_type": "execute_result"}], "source": "#Create RDD of array of numbers\nX = [\"1,2,3,4,5,6,7,8,9,10\"]\n\n#Note: Notice the numbers are in \"\" which keeps the values together.\n#Create an RDD\ny_rd = sc.parallelize(X)\n\n#Add Values 3 & 10\nSum_rd = y_rd.map(lambda y: y.split(\",\")).\\\nmap(lambda y: (int(y[2])+int(y[9])))\n\n#Note: Notice \\ to break line for code clarity\n#Note: Notice we used elements 2 and 9, array starts with 0\n#Return Sum Value\nSum_rd.first()"}, {"source": "##Creating an RDD from a data file\n\nApache Spark can access many data sources (Files, HDFS, APIs, Relational Data Sources, Etc.). These files need to be accessable by your Spark cluster.\n\nWe will use wget to pull Apache Spark README.md file into your fully managed spark cluster.", "cell_type": "markdown", "metadata": {}}, {"execution_count": 34, "cell_type": "code", "metadata": {"collapsed": false, "trusted": false}, "outputs": [{"output_type": "stream", "name": "stdout", "text": "--2016-03-15 14:33:56--  https://github.com/carloapp2/SparkPOT/blob/master/README.md\nResolving github.com (github.com)... 192.30.252.130\nConnecting to github.com (github.com)|192.30.252.130|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: unspecified [text/html]\nSaving to: \u2018README.md\u2019\n\n    [ <=>                                   ] 42,499      --.-K/s   in 0.04s   \n\n2016-03-15 14:33:56 (1.00 MB/s) - \u2018README.md\u2019 saved [42499]\n\n"}], "source": "!rm README.md* -f\n!wget https://github.com/carloapp2/SparkPOT/blob/master/README.md"}, {"source": "###Use SparkContext textFile to convert a text file to an RDD.", "cell_type": "markdown", "metadata": {}}, {"execution_count": 35, "cell_type": "code", "metadata": {"collapsed": false, "trusted": false}, "outputs": [{"execution_count": 35, "metadata": {}, "data": {"text/plain": "622"}, "output_type": "execute_result"}], "source": "#Put Data file into RDD\ntextfile_rdd = sc.textFile(\"/resources/README.md\")\ntextfile_rdd.count()"}, {"execution_count": 36, "cell_type": "code", "metadata": {"collapsed": false, "trusted": false}, "outputs": [{"output_type": "stream", "name": "stdout", "text": "The file README.md has the word SPARK in it 52 Times.\n"}], "source": "#Create a new RDD for all words \"Spark\" in text file\nSpark_rdd = textfile_rdd.filter(lambda line: \"Spark\" in line)\n\n#Print the count of elements in new RDD\nprint \"The file README.md has the word SPARK in it \" + str(Spark_rdd.count()) + ' Times.'"}, {"execution_count": null, "cell_type": "code", "metadata": {"collapsed": true, "trusted": false}, "outputs": [], "source": ""}], "metadata": {"kernelspec": {"display_name": "Python 2", "name": "python2", "language": "python"}, "language_info": {"mimetype": "text/x-python", "nbconvert_exporter": "python", "version": "2.7.11", "name": "python", "file_extension": ".py", "pygments_lexer": "ipython2", "codemirror_mode": {"version": 2, "name": "ipython"}}}, "nbformat": 4}